{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email classification with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, emails are clasified to two categories: messages that require a response, and messages that don't. The second category includes thank you and confirmation -messages among other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import sklearn\n",
    "import collections\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>correct_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cliff Good luck and more importantly, have fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan, Thank you for all of your help!! Lindy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you for your help in chasing down the ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are correct and it has been changed. Thank...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you very much for your help. Best, Jeff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  correct_class\n",
       "0  Cliff Good luck and more importantly, have fun...              1\n",
       "1        Jan, Thank you for all of your help!! Lindy              1\n",
       "2  Thank you for your help in chasing down the ma...              1\n",
       "3  You are correct and it has been changed. Thank...              1\n",
       "4      Thank you very much for your help. Best, Jeff              1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_excel(\"en_email_dataset.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3443, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(x=\"correct_class\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is the benchmark classifier that utilizes sklearn's pipeline to count the words in a dataset, performs tfidf-transforming, and classifies using SGD-classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       627\n",
      "           1       0.93      0.98      0.95       234\n",
      "\n",
      "    accuracy                           0.97       861\n",
      "   macro avg       0.96      0.98      0.97       861\n",
      "weighted avg       0.98      0.97      0.97       861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                        ('clf-svm', SGDClassifier(loss='log', penalty='l2',alpha=0.0001, max_iter=1000, random_state=42))])\n",
    "\n",
    "\n",
    "# Train model\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"message\"], df['correct_class'], random_state = 0)\n",
    "text_clf = text_clf.fit(x_train, y_train)\n",
    "\n",
    "# Perform analysis\n",
    "predicted = text_clf.predict(x_test)\n",
    "print(\"Accuracy: {}\".format(round(np.mean(predicted == y_test),2)))\n",
    "report = sklearn.metrics.classification_report(predicted, y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mr', 0.7017468214035034)\n",
      "('need', 0.8208699226379395)\n",
      "('get', 0.889103889465332)\n",
      "('files', 0.806323766708374)\n",
      "('sensitivity', 0.6831309199333191)\n",
      "('services', 0.8550636768341064)\n",
      "('supporting', 0.8092236518859863)\n",
      "('meet', 0.7461632490158081)\n"
     ]
    }
   ],
   "source": [
    "print(glove_model.most_similar(\"sir\")[0])\n",
    "print(glove_model.most_similar(\"help\")[0])\n",
    "print(glove_model.most_similar(\"need\")[0])\n",
    "print(glove_model.most_similar(\"file\")[0])\n",
    "print(glove_model.most_similar(\"attachment\")[0])\n",
    "print(glove_model.most_similar(\"service\")[0])\n",
    "print(glove_model.most_similar(\"support\")[0])\n",
    "print(glove_model.most_similar(\"meeting\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_list = list()\n",
    "emails = df[\"message\"].values.tolist()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "for email in emails:\n",
    "    tokens = word_tokenize(email)\n",
    "    # lowercase\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove puctuation\n",
    "    table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove non alphabetic tokens\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # remove stopwords\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    email_list.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word2vec model\n",
    "custom_model = gensim.models.Word2Vec(sentences=email_list, size=100, window=5, workers=4, min_count=2, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('otto', 0.7762136459350586)\n",
      "('casa', 0.47884416580200195)\n",
      "('kobra', 0.4627636969089508)\n",
      "('june', 0.6477605104446411)\n",
      "('harry', 0.6248341798782349)\n",
      "('palmnet', 0.7408140897750854)\n",
      "('court', 0.714199423789978)\n",
      "('board', 0.6633205413818359)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(custom_model.most_similar(\"sir\")[0])\n",
    "print(custom_model.most_similar(\"help\")[0])\n",
    "print(custom_model.most_similar(\"need\")[0])\n",
    "print(custom_model.most_similar(\"file\")[0])\n",
    "print(custom_model.most_similar(\"attachment\")[0])\n",
    "print(custom_model.most_similar(\"service\")[0])\n",
    "print(custom_model.most_similar(\"support\")[0])\n",
    "print(custom_model.most_similar(\"meeting\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "w2v = dict(zip(custom_model.wv.index2word, custom_model.wv.syn0))\n",
    "print(len(w2v))\n",
    "model.save(\"custom.model\")\n",
    "model.wv.save_word2vec_format('model.bin', binary=True)\n",
    "model.save(\"custom_model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the used dataset (3443 messages) is way too small to create a high quality word2vec model, and the results of the custom model are off a lot compared to the GloVe model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm preprocesses a dataset, builds a word2vec model of the processed data, and performs classification using MeanEmbeddingVectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model):\n",
    "        self.word_model = word_model\n",
    "        self.vector_size = word_model.wv.vector_size\n",
    "\n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):\n",
    "        doc_word_vector = self.word_average_list(docs)\n",
    "        return doc_word_vector\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        \"\"\"\n",
    "        Compute average word vector for a single doc/sentence.\n",
    "        :param sent: list of sentence tokens\n",
    "        :return:\n",
    "        mean: float of averaging word vectors\n",
    "        \"\"\"\n",
    "        mean = []\n",
    "        \n",
    "        for word in sent:\n",
    "            if word in self.word_model.wv.vocab:\n",
    "                mean.append(self.word_model.wv.get_vector(word))\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            # If a text is empty, return a vector of zeros.\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def word_average_list(self, docs):\n",
    "        \"\"\"\n",
    "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
    "        :param docs: list of sentence in list of separated tokens\n",
    "        :return:\n",
    "        array of average word vector in shape (len(docs),)\n",
    "        \"\"\"\n",
    "        return np.vstack([self.word_average(sent) for sent in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec_tr = MeanEmbeddingVectorizer(custom_model)\n",
    "doc_vec = mean_vec_tr.transform(df[\"message\"])\n",
    "text_clf = Pipeline([('clf-svm', SGDClassifier(loss='log', penalty='l2',alpha=0.0001, max_iter=1000, random_state=42))])\n",
    "x_train, x_test, y_train, y_test = train_test_split(doc_vec, df['correct_class'], random_state = 0)\n",
    "text_clf = text_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.95      0.66       333\n",
      "           1       0.93      0.43      0.59       528\n",
      "\n",
      "    accuracy                           0.63       861\n",
      "   macro avg       0.72      0.69      0.63       861\n",
      "weighted avg       0.77      0.63      0.62       861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform analysis\n",
    "predicted = text_clf.predict(x_test)\n",
    "print(\"Accuracy: {}\".format(round(np.mean(predicted == y_test),2)))\n",
    "report = sklearn.metrics.classification_report(predicted, y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "mean_vec_tr = MeanEmbeddingVectorizer(glove_model)\n",
    "doc_vec = mean_vec_tr.transform(df[\"message\"])\n",
    "text_clf = Pipeline([('clf-svm', SGDClassifier(loss='log', penalty='l2',alpha=0.0001, max_iter=1000, random_state=42))])\n",
    "x_train, x_test, y_train, y_test = train_test_split(doc_vec, df['correct_class'], random_state = 0)\n",
    "text_clf = text_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89       694\n",
      "           1       0.55      0.81      0.66       167\n",
      "\n",
      "    accuracy                           0.84       861\n",
      "   macro avg       0.75      0.83      0.78       861\n",
      "weighted avg       0.87      0.84      0.85       861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform analysis\n",
    "predicted = text_clf.predict(x_test)\n",
    "print(\"Accuracy: {}\".format(round(np.mean(predicted == y_test),2)))\n",
    "report = sklearn.metrics.classification_report(predicted, y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using embeddings turned out to be unsuccessful approach compared to more straightforward tfidf-vectorization. However, it is worth noticing the significant difference between the custom word2vec model that was built using a small dataset, and the GloVe model that has utilized billions of tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
